# Beyond Data Standards: Guest Lecture for LIS 546 (Spring 2021)

## Talk Outline 

1. Intro (Slide 1)
2. Outcomes (Slide 2)
3. What's a workflow? (Slides 3-5)
4. An everyday example: making coffee (inputs and outputs, and outputs that become inputs) (Slides 6-7)
5. Workflows: a formal definition + my definition (Slides 8-10)
6. Why workflows? Plus, a real-world example: next-generation sequencing (NGS) and a brief intro to bioinformatics (Slides 11-13)
7. The processing steps of sequencing (and why workflows are needed) -- and some caveats (Slide 14-15)
8. A crash-course on bioinformatics workflows (Slide 16)
9. But, what if I'm not interested in bioinformatics? Let's talk about general applications. (Slide 17)
10. 5 Workflow Implications for Data Curators (Slides 18-23)
11. Reflection Questions (Slide 24)
12. References, Resources, Workflow Tools, Learning Lab & Activities, How to Contact Me, Job Openings at Sage (Slides 25-30)

## Notes & Rough Script

1. Intro
2. Desired outcomes of mini lecture: 
- Understand workflows as both a concept and language 
- Articulate the importance of workflows in bioinformatics, and explain how they affect metadata
- Spark an interest in workflows, automation, and standardization beyond (meta)data standards

3. What is a workflow? _(Pause.)_
4. Then, start suggesting where workflows might already be occuring in daily life.
5. And then, move into an example. Take, for example: making coffee.
7. Our coffee-making workflow has multiple inputs and outputs. We could say it starts with the input of a currency, which produces the output of a bag of whole coffee beans. Which then itself becomes an input, with the output of ground coffee. Ground coffee then becomes our input into the coffee machine, which finally proudces the output of liquid gold, I mean, coffee. This idea of inputs and outputs, with outputs that then become inputs, can help us understand workflows. 
8. So again, what are workflows? While workflows can be as simple as describing how you flow through multiple work tasks, they can also be rather computationally complex, involving multiple programming languages, databases, inputs, outputs, and a lot of intervening steps. And that's where workflows come in when we're talking about data. You can also think of these as pipelines, though that tends to be associated with analysis. Whereas in bioinformatics, we need to do a good bit of processing, during a lengthy workflow, before we can begin to think about analysis.
9. So how about a definition? As you might imagine, it's a bit of a vague concept, so it's not easy to define, but I came across this one and liked how simple it was. So here we have it: a workflow is, "a sequence of operations to complete a process."
10. I also have my own definition, which is definitely biased by my current field: bioinformatics, but I think it helps distinguish this concept a bit more from general process documentation and data analysis pipelines. My definition is: ""using the outputs of one process, as the input for another, and chaining these steps together to form one large process that can be executed as one flow â€” ideally through one computational command."
11. All in all, though, they're both pretty simple definitions, but we're going to walk through a complex real-world example, using my current area of work -- bioinformatics and biomedical data curation. 
12. So you might be asking yourself: why workflows? Well, that's why we're going to use an example from the world of bioinformatics, because the answer has to do with a biological experimentation technique called next-generation, or 'next-gen,' sequencing -- or NGS, for short. There's also some other reasons we might care about workflows from a data curation perspective, and I'll get to those, after we talk about NGS.
13. So what's NGS?? I'll unpack this more in the next slide, and I've also linked a bunch of resources in the end notes, but you may already be familiar with it, due to the COVID-19 crisis. Lately, you might've heard NGS, or just 'sequencing,' come up in talk of COVID-19 variants. Variants refer to genomic variants of the virus, and we've discovered all the current ones through NGS, through a public health technique called genomic surveillance. Basically, scientists sequence the genome of the virus, looking for mutations, or variations (variants). These variants can prove difficult, but we're able to find out about them faster, as quickly as overnight, through NGS. COVID sucks, but NGS is pretty exciting.
14. Anyway, so in a nutshell, what is NGS? (Note: for brevity and simplicity, I'm going to skip over what's called 'library preparation' -- all the steps taken before actually running the sample through the sequencer. This part is important, too, and I'll briefly touch on it in a moment.) Basically, what happens during one kind of NGS that I'm familiar with, RNA sequencing (or RNA-seq), is a machine, called a sequencer, captures pictures of DNA base fragments (nucleotides) where we've extracted out mRNA, stacks upon stacks of them (literally millions!), and put this information into a huge file. The information collected in this step is referred to as 'unaligned reads.' It's basically just a binary file, which spits out the sequence of DNA base fragments (AGTC, etc.), and while it's nice, it doesn't tell us a lot. So we have to move on to the next step: aligning the base fragment readouts to a genome. This is eventually going to lead us to the next step, step three, where we count reads that match up to certain genes. From there, we're able to move into analysis, where we can figure out if certain genes are expressing certain things -- usually, by comparing 'normal' cells to diseased cells, allowing us to analyze patterns.
15. So that was a lot of steps, just to get to some data we might actually want to analyze. And as I hinted at, it doesn't even cover the preparation that goes into preparing a sample to go into the sequencer. Or even the sample itself, what kind of condition it's in, whether it's in a batch of samples with similar characteristics, etc. It also doesn't account for potential errors and/or failures in the experiment and sequencing machine itself. So we've got a lot of steps, and a lot of potential failure modes to correct or dispose of along the way, before we can get to the fun of analysis. And that's where workflows come in. They allow us to specify all these steps -- even if they're occurring in different languages and across systems -- and execute them all at once. Not only does that speed things up, it makes the process reproducible, and it allows us to share our work with others, and let others validate that work. 
16. Though, to note, it's not perfect. This xkcd is about data pipelines, but it applies to workflows, too. And not all workflows are the same either, which can cause issues, and there are, of course, multiple standards for workflows, too. 
17. Briefly, these are some of the workflow languages and systems in use for workflow management in bioinformatics. As a data curator, you likely won't have to actually know or run these, but your work will likely be adjacent to and complementing of this work, so it's useful to know about.
18. But at this point, you might be thinking: Nope. Not me. Biology isn't really my thing. First, that's fine -- we're going to talk about general implications for data curation next. And second: I didn't anticipate it would be my thing either, and yet here we are: I'm a bioinformatics analyst and data curator. :)
19. So take me to the general application! I've got five ways this applies to data curation at large, well beyond the world of bioinformatics.
20. First of all, there's data quality analysis metrics and data quality rubrics. This is going to come into play for you as a data curator, likely regardless of the kind of data. Almost all data, because it's produced by humans, needs a quality check. It just so happen biological data needs a lot of these! So this materializes in a lot of ways, through metadata attributes collecting quality information, and through data quality rubrics that you might ask contributors to adhere to, or sign statements assuring their data meets that standard. As the open data movement matures, this area seems to only be growing, and it's important for data curators to think about how they can confirm the data they're curating is accurate and high-quality, and how they can communicate this assurance to users. Whether a defined workflow is involved in this process, is up to you, but it's worth thinking about. 
21. This is a similar point to the first, but as data curators who care deeply about standards and reproducibility, it's important to be thinking about how to ensure reuse and provide provenance. And if that work can be automated, all the better, for both speed and error reduction. Additionally, laying out the data processing steps and publishing them in a way that others can re-run or re-reproduce can help you spot data bias as well as errors, and it can also help others criticize and spot these issues, too. This can lead to more equitable data practices. (Not always, of course, but it's a step in the right direction.) 
22. Another pro of workflows -- when specified in code or some other reproducible method -- they're often very portable and interoperable. The interoperability piece should resonate with data curators, as we all want to adhere to FAIR principles where possible.
23. Workflows also highlight an area of increasing interest in data curation: data validation. Data often needs to be validated before we ingest it, to ensure it conforms to metadata standards, controlled terminologies, and other specifications. Setting up clear, documented, and ideally automated workflows to do this is a way to advance your data curation work.
24. Finally, an area where everyone can interact with the concept of workflows is in good documentation. Even if you're not a strong programmer, and even if you're sure your interest level in bioinformatics is zero, one path to incorporating workflows into your data curation repertoire is through excellent documentation. Clearly explaining and documenting how you curate, and how you do other work associated with data curation, can help colleagues, contributors, and users execute consistent, reusable workflows. (And pro tip: if you want to get started with a more low-key form of automating your various workflows, consider learning some bash scripting in the command line!)
25. Finally: some reflection questions: 
- What workflow could you start to standardize today?
- How can you apply the concept of portable, repeatable, standardized workflows to your data curation work?
- How can published workflows reduce data bias?
- What would a data curation workflow language look like?  What kinds of commands would you want to execute?

26. Check out the references, resources, activities, etc. [here](https://github.com/kthrog/LIS-546-guest-lecture/blob/main/files/resources.md). 
